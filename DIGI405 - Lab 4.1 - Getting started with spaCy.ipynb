{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIGI405 Lab 4.1: Getting started with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For advanced text processing capabilities, the [spaCy](https://spacy.io/) NLP library offers a range of features. The aims of this notebook are to introduce spaCy and demonstrate basic parts of speech tagging and dependency parsing methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing spaCy\n",
    "\n",
    "The spaCy library and small language model are already installed on JupyterHub. However, if you are running the notebook locally, you may need to install them. You can find the instructions in the spaCy documentation here: [https://spacy.io/usage](https://spacy.io/usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import collections\n",
    "from collections import Counter\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Sometime between 1250 and 1300, Polynesians settled in the islands that later were named New Zealand and developed a distinctive Māori culture. \n",
    "In 1642, Dutch explorer Abel Tasman became the first European to sight New Zealand. \n",
    "In 1840, representatives of the United Kingdom and Māori chiefs signed the Treaty of Waitangi, which declared British sovereignty over the islands. \n",
    "In 1841, New Zealand became a colony within the British Empire and in 1907 it became a dominion; it gained full statutory independence in 1947 and the British monarch remained the head of state.\n",
    "'''\n",
    "# text from https://en.wikipedia.org/wiki/New_Zealand\n",
    "# you can replace the text variable with any text you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell tokenises and annotates some text with spacy and prints out the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise and annotate some text\n",
    "doc = nlp(text) \n",
    "\n",
    "# this will output the individual tokens \n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy can segment our text into sentences. This cell prints each sentence in turn ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech tagging with spaCy\n",
    "\n",
    "There are two ways to access parts of speech in spaCy. First we will output a count of parts of speech using Penn Treebank tags. \n",
    "\n",
    "The [spacy.explain](https://spacy.io/api/top-level#spacy.explain) function can be used to output a user-friendly description for a given POS tag, dependency label or entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [token.tag_ for token in doc]\n",
    "\n",
    "tag_freq = Counter(tags)\n",
    "for tag in sorted(tag_freq, key=tag_freq.get, reverse=True):\n",
    "    print(tag, tag_freq[tag], spacy.explain(tag), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy also uses the simpler set of 'Universal' part of speech labels. These are also very useful for general POS patterns.\n",
    "\n",
    "Note the difference between the cell above and below (i.e. above the Penn Treebank tags are accessed via tag_ and below Spacy's Universal POS tags are available via pos_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [token.pos_ for token in doc]\n",
    "\n",
    "tag_freq = Counter(tags)\n",
    "for tag in sorted(tag_freq, key=tag_freq.get, reverse=True):\n",
    "    print(tag, tag_freq[tag], spacy.explain(tag), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of filtering tokens by their part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == 'PROPN':\n",
    "        filtered_tokens.append(token)\n",
    "        \n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the tags listed above with their spacy.explain explanations. Replace 'PROPN' with other parts of speech, such as:\n",
    "\n",
    "- VERB\n",
    "- NOUN\n",
    "- ADP (preposition)\n",
    "- NUM (numbers)\n",
    "\n",
    "You can also use the more extensive Penn Treebank tag set under the \"English\" heading. This allows you to do things like differentiate between tenses of verbs (e.g. select verbs in past-tense, VBD). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.tag_ == 'VBD': # similar to above, except instead of pos_ we use tag_ to access the different tag set\n",
    "        filtered_tokens.append(token)\n",
    "        \n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are outputting a frequency list for proper nouns. Change the code to output a frequency list for another part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select prop noun tokens only \n",
    "filtered_tokens = [token.text for token in doc if token.pos_ == \"PROPN\"]\n",
    "\n",
    "token_freq = Counter(filtered_tokens)\n",
    "for token in sorted(token_freq, key=token_freq.get, reverse=True):\n",
    "    print(token, token_freq[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this model to remove or normalise particular token types as required. For example, here we are normalising individual numbers to one token \"NUMBER\". This might be useful if you were interested in collocation patterns related to numbers in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_numbers = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == 'NUM':\n",
    "        no_numbers.append('NUMBER') # if we wanted to remove numbers completely change this line to: continue\n",
    "    else:\n",
    "        no_numbers.append(token)\n",
    "\n",
    "print(no_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering by character types of tokens\n",
    "\n",
    "You can filter by types of tokens. For example, here we are excluding any tokens with non-alphabetic characters such as numbers or '$'. Look at the list of token attributes here: https://spacy.io/api/token#attributes  \n",
    "Change is_alpha to another boolean (bool) type to filter in another way (e.g. is_digit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_filtered = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.is_alpha is False:\n",
    "        continue\n",
    "    else:\n",
    "        char_filtered.append(token) \n",
    "\n",
    "print(char_filtered)\n",
    "# no dates or punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun phrases / chunks\n",
    "\n",
    "Identifying noun chunks is a basic way of identifying entities and objects written about in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency parsing\n",
    "\n",
    "Dependency parsing analyses sentences based on relationships between words.\n",
    "\n",
    "You can view the different annotation labels for Spacy's dependency parsing here: https://spacy.io/models/en#en_core_web_sm (expand the \"label scheme\" and see the PARSER labels). \n",
    "\n",
    "Spacy is packaged with a useful dependency visualiser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of sentences\n",
    "sentences = list(doc.sents) # create a list of sentences\n",
    "# the sentences list can be passed to the following line, but here just displaying the shortest sentence\n",
    "html = spacy.displacy.render(sentences[1], style='dep', jupyter = False)\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (ipykernel)",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
